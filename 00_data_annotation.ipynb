{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52ecfe7",
   "metadata": {},
   "source": [
    "<h1>App-Klassifikation f√ºr Life Sciences</h1>\n",
    "\n",
    "**Ziel:**\n",
    "Baue einen einfachen Python-Klassifikator in einem Low-Resource-Setting, um Apps zu identifizieren, die f√ºr die Branchen Life Science, MedTech und Pharma relevant sein k√∂nnten oder welche Apps GxP relevant sind.\n",
    "\n",
    "**Anforderungen:**\n",
    "\n",
    "- Verwende ein Jupyter Notebook.\n",
    "- Nutze die im Anhang bereitgestellten Beispieldaten als Grundlage.\n",
    "- Entwickle mindestens zwei Klassifikationsmethoden (z.‚ÄØB. einfache ML-Modelle, Vektorans√§tze).\n",
    "- Annotiere und pr√ºfe deine Daten.\n",
    "- Visualisiere deine Ergebnisse (z.‚ÄØB. Konfusionsmatrix, Embedding-Plots, Feature-Importance).\n",
    "- Gib die wichtigsten Merkmale f√ºr deine Klassifikation an.\n",
    "- Erkl√§re deine Vorgehensweise und Methodenwahl.\n",
    "\n",
    "**Optionale Tools:**\n",
    "\n",
    "- pandas, numpy, scikit-learn, weaviate\n",
    "- fasttext, word2vec\n",
    "\n",
    "**Zus√§tzliche Empfehlungen:**\n",
    "\n",
    "- Nutze ggf. Transfer Learning oder Embedding-Techniken, um mit wenigen Beispielen zu arbeiten.\n",
    "- Begr√ºnde deine Modellwahl (z.‚ÄØB. warum ein bestimmter Klassifikator f√ºr dieses Setting geeignet ist).\n",
    "- Achte auf Nachvollziehbarkeit und Reproduzierbarkeit deines Codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad48d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/keen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/keen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imported necessary libraries\n",
    "from google_play_scraper import search, app # Life Sciences App Scraper using google-play-scraper\n",
    "from app_store_scraper import AppStore # Life Sciences App Scraper using apple-scraper\n",
    "import pandas as pd\n",
    "import time # to avoid anti-scraper\n",
    "import requests\n",
    "# \n",
    "# tf-idf to collect keywords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "#prune the keyword list\n",
    "import fasttext\n",
    "\n",
    "# gensim Word to vector\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# \n",
    "nltk.download('punkt')\n",
    "\n",
    "#filter out ill formed data\n",
    "import re\n",
    "\n",
    "# for data annotation\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af3114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apple version: collect data from apple store, using keyword list:\n",
    "def fetch_apple_store_apps(keywords, lang='en', country='US', file_suffix='round0'):\n",
    "    \"\"\"\n",
    "    from Apple App Store, fetch data \n",
    "\n",
    "    Parameters:\n",
    "    1, keywords: \n",
    "    2, lang: default as 'en'\n",
    "    3, country: default as 'US'\n",
    "    4, file_suffix: csv file name: default as'round0'\n",
    "\n",
    "    return:\n",
    "    1, pandas DataFrame\n",
    "    \"\"\"\n",
    "    app_data = []\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(f\"Searching Apple Store for: {kw}\")\n",
    "        try:\n",
    "            url = f\"https://itunes.apple.com/search?term={kw}&entity=software&country={country}&lang={lang}&limit=50\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "\n",
    "            if data['resultCount'] == 0:\n",
    "                print(f\"  No results for keyword: {kw}\")\n",
    "                continue\n",
    "\n",
    "            for app in data['results']:\n",
    "                app_data.append({\n",
    "                    \"trackId\": app.get(\"trackId\"),\n",
    "                    \"title\": app.get(\"trackName\"),\n",
    "                    \"description\": app.get(\"description\"),\n",
    "                    \"genre\": app.get(\"primaryGenreName\"),\n",
    "                    \"developer\": app.get(\"sellerName\"),\n",
    "                    \"url\": app.get(\"trackViewUrl\"),\n",
    "                    \"rating\": app.get(\"averageUserRating\"),\n",
    "                    \"ratingCount\": app.get(\"userRatingCount\"),\n",
    "                    \"keyword\": kw\n",
    "                })\n",
    "            print(f\"  ‚úì Found {len(data['results'])} apps for keyword '{kw}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"  √ó Error searching keyword {kw}: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    # create DataFrame,remove duplication\n",
    "    if app_data:\n",
    "        df = pd.DataFrame(app_data)\n",
    "        df = df.drop_duplicates(subset=['trackId'])\n",
    "\n",
    "        # construct csv file name\n",
    "        filename = f\"{file_suffix}_apple_{lang}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"‚úÖ Saved data to {filename}, total apps after deduplication: {len(df)}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data collected.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24b8d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#google version: collect data from google store, using keyword list:\n",
    "def fetch_google_play_apps(keywords, lang='en', country='us', file_suffix='round0'):\n",
    "    \"\"\"\n",
    "    from google play, fetch data \n",
    "\n",
    "    Parameters:\n",
    "    1, keywords: \n",
    "    2, lang: default as 'en'\n",
    "    3, country: default as 'US'\n",
    "    4, file_suffix: csv file name: default as'round0'\n",
    "\n",
    "    return:\n",
    "    1, pandas DataFrame\n",
    "    \"\"\"\n",
    "    app_data = []\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(f\"Searching Google Play for: {kw}\")\n",
    "        try:\n",
    "            results = search(kw, lang=lang, country=country, n_hits=50)\n",
    "        except Exception as e:\n",
    "            print(f\"  √ó Search failed for keyword '{kw}': {e}\")\n",
    "            continue\n",
    "\n",
    "        for res in results:\n",
    "            try:\n",
    "                app_info = app(res['appId'], lang=lang, country=country)\n",
    "                app_data.append({\n",
    "                    \"appId\": app_info.get(\"appId\"),\n",
    "                    \"title\": app_info.get(\"title\"),\n",
    "                    \"description\": app_info.get(\"description\"),\n",
    "                    \"genre\": app_info.get(\"genre\"),\n",
    "                    \"score\": app_info.get(\"score\"),\n",
    "                    \"installs\": app_info.get(\"installs\"),\n",
    "                    \"developer\": app_info.get(\"developer\"),\n",
    "                    \"url\": app_info.get(\"url\"),\n",
    "                    \"keyword\": kw\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"  √ó Error fetching details for appId '{res.get('appId', 'N/A')}': {e}\")\n",
    "            time.sleep(1)  # avoid to be blocked\n",
    "\n",
    "    # get DataFrame, remove duplication\n",
    "    if app_data:\n",
    "        df = pd.DataFrame(app_data)\n",
    "        df = df.drop_duplicates(subset=['appId'])\n",
    "\n",
    "        # construct csv file name\n",
    "        filename = f\"{file_suffix}_googleplay_{lang}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"‚úÖ Saved data to {filename}, total apps after deduplication: {len(df)}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data collected.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48060307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Google Play for: Life Science\n",
      "Searching Google Play for: MedTech\n",
      "Searching Google Play for: pharma\n",
      "Searching Google Play for: GxP\n",
      "‚úÖ Saved data to round0_googleplay_en.csv, total apps after duplication: 118\n",
      "Searching Apple Store for: Life Science\n",
      "  ‚úì Found 50 apps for keyword 'Life Science'\n",
      "Searching Apple Store for: MedTech\n",
      "  ‚úì Found 41 apps for keyword 'MedTech'\n",
      "Searching Apple Store for: pharma\n",
      "  ‚úì Found 46 apps for keyword 'pharma'\n",
      "Searching Apple Store for: GxP\n",
      "  ‚úì Found 7 apps for keyword 'GxP'\n",
      "‚úÖ Saved data to round0_apple_en.csv, total apps after deduplication: 141\n"
     ]
    }
   ],
   "source": [
    "initial_keywords = [\"Life Science\", \"MedTech\", \"pharma\", \"GxP\"]\n",
    "df_round_0_google = fetch_google_play_apps(initial_keywords, lang='en', country='us', file_suffix='round0')\n",
    "df_round_0_apple = fetch_apple_store_apps(initial_keywords, lang='en', country='us', file_suffix='round0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b69fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_new_kw_tfidf(df, text_column='description', existing_keywords=None, lang='english', max_features=100):\n",
    "    \"\"\"\n",
    "    extract TF-IDF keywords from DataFrame's description, return a keyword list remove the existing keywords\n",
    "\n",
    "    parameters:\n",
    "    1, df: pandas DataFrame\n",
    "    2, text_column: for extraction, default as 'description'\n",
    "    3, existing_keywords: optional, if already exist some keywords\n",
    "    4, lang: for stopwords, default as 'english'\n",
    "    5, max_features: at most perserved feature number, default as 100\n",
    "\n",
    "    return:\n",
    "    1, new_keywords: list\n",
    "    \"\"\"\n",
    "    # 1. check \n",
    "    if existing_keywords is None:\n",
    "        existing_keywords = []\n",
    "\n",
    "    # 2. create TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words=stopwords.words(lang)\n",
    "    )\n",
    "\n",
    "    # 3. dump nan data\n",
    "    X = vectorizer.fit_transform(df[text_column].dropna())\n",
    "\n",
    "    # 4. get feature\n",
    "    tfidf_keywords = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # 5. remove duplication\n",
    "    new_keywords = list(set(tfidf_keywords) - set(existing_keywords))\n",
    "\n",
    "    # 6. print first 100 keyworda\n",
    "    print(f\"‚úÖ Extracted {len(new_keywords)} new keywords (top 100 shown):\")\n",
    "    print(new_keywords[:100])\n",
    "\n",
    "    return new_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86876a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 99 new keywords (top 100 shown):\n",
      "['clinical', 'order', 'support', 'care', 'using', 'search', 'treatment', 'including', 'www', 'https', 'take', 'save', 'gps', 'papers', 'exams', 'medical', 'find', 'time', 'employee', 'drug', 'help', 'one', 'medicine', 'use', 'hotels', 'rewards', 'medicines', 'comprehensive', 'professionals', 'best', 'terms', 'app', 'every', 'exam', 'available', 'information', 'anywhere', 'com', 'offers', 'offline', 'drugs', 'real', 'whether', 'biology', 'like', 'plan', 'test', 'user', 'download', 'topics', 'view', 'need', 'maps', 'store', 'pharmacy', 'choose', 'get', 'us', 'life', 'tools', 'access', 'questions', 'healthcare', 'easily', 'prescription', 'features', 'mobile', 'experience', 'li', 'products', 'learning', 'based', 'learn', 'subscription', 'news', 'content', 'stay', 'practice', 'easy', 'new', 'key', 'make', 'online', 'card', 'manage', 'management', 'also', 'science', 'provides', 'track', 'data', 'free', 'study', 'application', 'knowledge', 'health', 'map', 'conditions', 'students']\n",
      "‚úÖ Extracted 99 new keywords (top 100 shown):\n",
      "['support', 'care', 'using', 'including', 'www', 'https', 'take', 'designed', '24', 'go', 'system', 'mg', 'medical', 'different', 'find', 'time', 'users', 'help', 'one', 'use', 'space', 'professionals', 'best', 'terms', 'app', 'every', 'daily', 'information', 'com', 'real', 'like', 'de', 'test', 'user', 'insights', 'latest', 'download', 'view', 'kids', 'game', 'need', 'medtech', 'pharmaceutical', 'pharmacy', 'get', 'earth', 'life', 'educational', 'platform', 'tools', 'complete', 'field', 'access', 'interactive', 'grandpa', 'healthcare', 'features', 'policy', 'mobile', 'create', 'experience', 'products', 'school', 'privacy', 'etc', 'learning', 'based', 'learn', 'subscription', 'news', 'account', 'content', 'share', 'food', 'explore', 'easy', 'new', 'play', 'make', 'online', 'manage', 'apps', 'also', 'science', 'industry', 'world', 'event', 'fun', 'games', 'evolution', 'data', 'free', 'study', 'service', 'application', 'knowledge', 'health', 'home', 'students']\n",
      "['clinical', 'order', 'support', 'care', 'using', 'search', 'treatment', 'including', 'www', 'https', 'take', 'save', 'gps', 'papers', 'exams', 'medical', 'find', 'time', 'employee', 'drug', 'help', 'one', 'medicine', 'use', 'hotels', 'rewards', 'medicines', 'comprehensive', 'professionals', 'best', 'terms', 'app', 'every', 'exam', 'available', 'information', 'anywhere', 'com', 'offers', 'offline', 'drugs', 'real', 'whether', 'biology', 'like', 'plan', 'test', 'user', 'download', 'topics', 'view', 'need', 'maps', 'store', 'pharmacy', 'choose', 'get', 'us', 'life', 'tools', 'access', 'questions', 'healthcare', 'easily', 'prescription', 'features', 'mobile', 'experience', 'li', 'products', 'learning', 'based', 'learn', 'subscription', 'news', 'content', 'stay', 'practice', 'easy', 'new', 'key', 'make', 'online', 'card', 'manage', 'management', 'also', 'science', 'provides', 'track', 'data', 'free', 'study', 'application', 'knowledge', 'health', 'map', 'conditions', 'students']\n",
      "['support', 'care', 'using', 'including', 'www', 'https', 'take', 'designed', '24', 'go', 'system', 'mg', 'medical', 'different', 'find', 'time', 'users', 'help', 'one', 'use', 'space', 'professionals', 'best', 'terms', 'app', 'every', 'daily', 'information', 'com', 'real', 'like', 'de', 'test', 'user', 'insights', 'latest', 'download', 'view', 'kids', 'game', 'need', 'medtech', 'pharmaceutical', 'pharmacy', 'get', 'earth', 'life', 'educational', 'platform', 'tools', 'complete', 'field', 'access', 'interactive', 'grandpa', 'healthcare', 'features', 'policy', 'mobile', 'create', 'experience', 'products', 'school', 'privacy', 'etc', 'learning', 'based', 'learn', 'subscription', 'news', 'account', 'content', 'share', 'food', 'explore', 'easy', 'new', 'play', 'make', 'online', 'manage', 'apps', 'also', 'science', 'industry', 'world', 'event', 'fun', 'games', 'evolution', 'data', 'free', 'study', 'service', 'application', 'knowledge', 'health', 'home', 'students']\n"
     ]
    }
   ],
   "source": [
    "keywords_google_gensim = extract_new_kw_tfidf(df_round_0_google,'description',initial_keywords,'english')\n",
    "keywords_apple_gensim = extract_new_kw_tfidf(df_round_0_apple,'description',initial_keywords,'english')\n",
    "print(keywords_google_gensim)\n",
    "print(keywords_apple_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c39a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_keywords_google_gensim = ['medication','medicine','pharmacy','clinical','healthcare','prescription','medical','biology','science']\n",
    "clean_keywords_apple_gensim = ['pharmacy','medtech','healthcare','medical','health','pharmaceutical']\n",
    "clean_keywords_combine_gensim =  ['medication','medicine','pharmacy','clinical','healthcare','prescription','medical','biology','science','medtech','pharmaceutical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08fbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_expand_kw_word2vec(df, text_column='description',\n",
    "                                       keywords_to_check=None,\n",
    "                                       lang='english',\n",
    "                                       vector_size=50, window=5,\n",
    "                                       min_count=1, workers=4,\n",
    "                                       topn=10):\n",
    "    \"\"\"\n",
    "    train Word2Vec model, get similar words for each keywords,return model and all similar word list\n",
    "\n",
    "    parameters:\n",
    "    1, df: DataFrame\n",
    "    2, text_column: default as description\n",
    "    3, keywords_to_check: keyword list\n",
    "    4, lang: stopword language 'english'\n",
    "    5, vector_size, window, min_count, workers: Word2Vec parameter of the model \n",
    "    6, topn: number of similar words, default as 10\n",
    "\n",
    "    return:\n",
    "    1, model: trained Word2Vec model\n",
    "    2, expanded_keywords: list[str]\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üìò preparing stopwords and data...\")\n",
    "    stop_words = set(stopwords.words(lang))\n",
    "    corpus = []\n",
    "    for doc in df[text_column].dropna():\n",
    "        tokens = word_tokenize(str(doc).lower())\n",
    "        filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "        if filtered_tokens:\n",
    "            corpus.append(filtered_tokens)\n",
    "\n",
    "    print(f\"‚úÖ preprocess finished, {len(corpus)} corpus in total.\")\n",
    "\n",
    "    print(\"üöÄ trainning Word2Vec model...\")\n",
    "    model = Word2Vec(sentences=corpus,\n",
    "                     vector_size=vector_size,\n",
    "                     window=window,\n",
    "                     min_count=min_count,\n",
    "                     workers=workers)\n",
    "    print(\"‚úÖ training completed\")\n",
    "\n",
    "    all_words = [w for doc in corpus for w in doc]\n",
    "    expanded_keywords = set()\n",
    "\n",
    "    if keywords_to_check:\n",
    "        for kw in keywords_to_check:\n",
    "            print(f\"\\nüîç keywords:'{kw}'\")\n",
    "            count = all_words.count(kw)\n",
    "            print(f\"  appearing time: {count}\")\n",
    "            try:\n",
    "                similar_words = model.wv.most_similar(kw, topn=topn)\n",
    "                print(\"similar words:\")\n",
    "                for word, score in similar_words:\n",
    "                    print(f\"    {word} ({score:.2f})\")\n",
    "                    expanded_keywords.add(word)\n",
    "                expanded_keywords.add(kw)  # added original keywords\n",
    "            except KeyError:\n",
    "                print(f\"‚ö†Ô∏è '{kw}' not included in the glossary\")\n",
    "\n",
    "    # transfor as list\n",
    "    expanded_keywords = sorted(list(expanded_keywords))\n",
    "    print(f\"\\nüì¶ return keywords number: {len(expanded_keywords)} \")\n",
    "\n",
    "    return model, expanded_keywords\n",
    "\n",
    "# prune by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c29f209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò preparing stopwords and data...\n",
      "‚úÖ preprocess finished, 118 corpus in total.\n",
      "üöÄ trainning Word2Vec model...\n",
      "‚úÖ training completed\n",
      "\n",
      "üîç keywords:'medical'\n",
      "  appearing time: 134\n",
      "similar words:\n",
      "    app (0.97)\n",
      "    test (0.97)\n",
      "    access (0.96)\n",
      "    get (0.96)\n",
      "    care (0.96)\n",
      "    pharmacy (0.96)\n",
      "    find (0.95)\n",
      "    exam (0.95)\n",
      "    medicine (0.95)\n",
      "    use (0.95)\n",
      "    learn (0.95)\n",
      "    help (0.95)\n",
      "    students (0.95)\n",
      "    features (0.95)\n",
      "    manage (0.95)\n",
      "    information (0.95)\n",
      "    life (0.95)\n",
      "    time (0.95)\n",
      "    also (0.95)\n",
      "    free (0.95)\n",
      "    maps (0.95)\n",
      "    drug (0.95)\n",
      "    online (0.95)\n",
      "    healthcare (0.94)\n",
      "    need (0.94)\n",
      "    practice (0.94)\n",
      "    health (0.94)\n",
      "    like (0.94)\n",
      "    questions (0.94)\n",
      "    search (0.94)\n",
      "\n",
      "üì¶ return keywords number: 31 \n",
      "üìò preparing stopwords and data...\n",
      "‚úÖ preprocess finished, 140 corpus in total.\n",
      "üöÄ trainning Word2Vec model...\n",
      "‚úÖ training completed\n",
      "\n",
      "üîç keywords:'medical'\n",
      "  appearing time: 66\n",
      "similar words:\n",
      "    test (0.55)\n",
      "    medtech (0.52)\n",
      "    e (0.50)\n",
      "    new (0.50)\n",
      "    data (0.49)\n",
      "    intentional (0.48)\n",
      "    visualmente (0.48)\n",
      "    manage (0.47)\n",
      "    learning (0.47)\n",
      "    app (0.47)\n",
      "    time (0.46)\n",
      "    posi≈Çkowa (0.46)\n",
      "    check (0.46)\n",
      "    play (0.45)\n",
      "    discover (0.45)\n",
      "    easy (0.44)\n",
      "    topics (0.44)\n",
      "    random (0.44)\n",
      "    fee (0.44)\n",
      "    learn (0.44)\n",
      "\n",
      "üì¶ return keywords number: 21 \n"
     ]
    }
   ],
   "source": [
    "model_google,keywords_google_w2v = extract_expand_kw_word2vec(df_round_0_google, text_column='description',\n",
    "                                       keywords_to_check=[\"medical\"],\n",
    "                                       lang='english',\n",
    "                                       vector_size=50, window=5,\n",
    "                                       min_count=1, workers=4,\n",
    "                                       topn=30)\n",
    "model_apple,keywords_apple_w2v = extract_expand_kw_word2vec(df_round_0_apple, text_column='description',\n",
    "                                       keywords_to_check=[\"medical\"],\n",
    "                                       lang='english',\n",
    "                                       vector_size=50, window=5,\n",
    "                                       min_count=1, workers=4,\n",
    "                                       topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d81c8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'also', 'app', 'care', 'drug', 'exam', 'features', 'find', 'free', 'get', 'health', 'healthcare', 'help', 'information', 'learn', 'life', 'like', 'manage', 'maps', 'medical', 'medicine', 'need', 'online', 'pharmacy', 'practice', 'questions', 'search', 'students', 'test', 'time', 'use']\n",
      "['app', 'check', 'data', 'discover', 'e', 'easy', 'fee', 'intentional', 'learn', 'learning', 'manage', 'medical', 'medtech', 'new', 'play', 'posi≈Çkowa', 'random', 'test', 'time', 'topics', 'visualmente']\n"
     ]
    }
   ],
   "source": [
    "print(keywords_google_w2v)\n",
    "print(keywords_apple_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0e50be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Google Play for: medication\n",
      "Searching Google Play for: medicine\n",
      "Searching Google Play for: pharmacy\n",
      "Searching Google Play for: clinical\n",
      "Searching Google Play for: healthcare\n",
      "Searching Google Play for: prescription\n",
      "Searching Google Play for: medical\n",
      "Searching Google Play for: biology\n",
      "Searching Google Play for: science\n",
      "Searching Google Play for: medtech\n",
      "Searching Google Play for: pharmaceutical\n",
      "‚úÖ Saved data to round1_googleplay_en.csv, total apps after duplication: 246\n",
      "Searching Apple Store for: medication\n",
      "  ‚úì Found 51 apps for keyword 'medication'\n",
      "Searching Apple Store for: medicine\n",
      "  ‚úì Found 55 apps for keyword 'medicine'\n",
      "Searching Apple Store for: pharmacy\n",
      "  ‚úì Found 51 apps for keyword 'pharmacy'\n",
      "Searching Apple Store for: clinical\n",
      "  ‚úì Found 50 apps for keyword 'clinical'\n",
      "Searching Apple Store for: healthcare\n",
      "  ‚úì Found 53 apps for keyword 'healthcare'\n",
      "Searching Apple Store for: prescription\n",
      "  ‚úì Found 53 apps for keyword 'prescription'\n",
      "Searching Apple Store for: medical\n",
      "  ‚úì Found 59 apps for keyword 'medical'\n",
      "Searching Apple Store for: biology\n",
      "  ‚úì Found 50 apps for keyword 'biology'\n",
      "Searching Apple Store for: science\n",
      "  ‚úì Found 51 apps for keyword 'science'\n",
      "Searching Apple Store for: medtech\n",
      "  ‚úì Found 41 apps for keyword 'medtech'\n",
      "Searching Apple Store for: pharmaceutical\n",
      "  ‚úì Found 32 apps for keyword 'pharmaceutical'\n",
      "‚úÖ Saved data to round1_apple_en.csv, total apps after deduplication: 454\n"
     ]
    }
   ],
   "source": [
    "df_round_1_google = fetch_google_play_apps(clean_keywords_combine_gensim, lang='en', country='us', file_suffix='round1')\n",
    "df_round_1_apple = fetch_apple_store_apps(clean_keywords_combine_gensim, lang='en', country='us', file_suffix='round1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad220d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Google Play for: Life Science\n",
      "Searching Google Play for: MedTech\n",
      "Searching Google Play for: pharma\n",
      "Searching Google Play for: GxP\n",
      "Searching Google Play for: medication\n",
      "Searching Google Play for: medicine\n",
      "Searching Google Play for: pharmacy\n",
      "Searching Google Play for: clinical\n",
      "Searching Google Play for: healthcare\n",
      "Searching Google Play for: prescription\n",
      "Searching Google Play for: medical\n",
      "Searching Google Play for: biology\n",
      "Searching Google Play for: science\n",
      "Searching Google Play for: medtech\n",
      "Searching Google Play for: pharmaceutical\n",
      "‚úÖ Saved data to done_googleplay_en.csv, total apps after duplication: 318\n",
      "Searching Apple Store for: Life Science\n",
      "  ‚úì Found 50 apps for keyword 'Life Science'\n",
      "Searching Apple Store for: MedTech\n",
      "  ‚úì Found 41 apps for keyword 'MedTech'\n",
      "Searching Apple Store for: pharma\n",
      "  ‚úì Found 46 apps for keyword 'pharma'\n",
      "Searching Apple Store for: GxP\n",
      "  ‚úì Found 7 apps for keyword 'GxP'\n",
      "Searching Apple Store for: medication\n",
      "  ‚úì Found 51 apps for keyword 'medication'\n",
      "Searching Apple Store for: medicine\n",
      "  ‚úì Found 55 apps for keyword 'medicine'\n",
      "Searching Apple Store for: pharmacy\n",
      "  ‚úì Found 51 apps for keyword 'pharmacy'\n",
      "Searching Apple Store for: clinical\n",
      "  ‚úì Found 50 apps for keyword 'clinical'\n",
      "Searching Apple Store for: healthcare\n",
      "  ‚úì Found 53 apps for keyword 'healthcare'\n",
      "Searching Apple Store for: prescription\n",
      "  ‚úì Found 53 apps for keyword 'prescription'\n",
      "Searching Apple Store for: medical\n",
      "  ‚úì Found 59 apps for keyword 'medical'\n",
      "Searching Apple Store for: biology\n",
      "  ‚úì Found 50 apps for keyword 'biology'\n",
      "Searching Apple Store for: science\n",
      "  ‚úì Found 51 apps for keyword 'science'\n",
      "Searching Apple Store for: medtech\n",
      "  ‚úì Found 41 apps for keyword 'medtech'\n",
      "Searching Apple Store for: pharmaceutical\n",
      "  ‚úì Found 32 apps for keyword 'pharmaceutical'\n",
      "‚úÖ Saved data to done_apple_en.csv, total apps after deduplication: 546\n"
     ]
    }
   ],
   "source": [
    "clean_keywords_combine =  [\"Life Science\", \"MedTech\", \"pharma\", \"GxP\",'medication','medicine','pharmacy','clinical','healthcare','prescription','medical','biology','science','medtech','pharmaceutical']\n",
    "df_done_google = fetch_google_play_apps(clean_keywords_combine, lang='en', country='us', file_suffix='done')\n",
    "df_done_apple = fetch_apple_store_apps(clean_keywords_combine, lang='en', country='us', file_suffix='done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5153557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# show all col\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# row\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "'''\n",
    "\n",
    "\n",
    "# display setting\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9ba3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_google_play_links(csv_path, column_name='trackId', output_path=None):\n",
    "    \"\"\"\n",
    "    read CSV, keep the data that fits Google Play's format\n",
    "\n",
    "    parameters\n",
    "    1, csv_path (str): CSV file path\n",
    "    2, column_name (str)\n",
    "    3, output_path (str, optional)\n",
    "\n",
    "    return:\n",
    "    4, pd.DataFrame: clean DataFrame\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # define Google Play link's regulation\n",
    "    pattern = re.compile(r'^https:\\/\\/play\\.google\\.com\\/store\\/apps\\/details\\?id=[\\w\\.]+(?:&[\\w=]*)*$')\n",
    "\n",
    "    # filter using bool mask\n",
    "    mask = df[column_name].apply(lambda x: bool(pattern.match(str(x).strip())))\n",
    "    df_filtered = df[mask].copy()\n",
    "\n",
    "    # store the clean data\n",
    "    if output_path:\n",
    "        df_filtered.to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ Filtered file saved to: {output_path}\")\n",
    "\n",
    "    print(f\"‚úÖ {len(df_filtered)} valid Google Play links retained.\")\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e8dfcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filtered file saved to: clean_google_en.csv\n",
      "‚úÖ 318 valid Google Play links retained.\n"
     ]
    }
   ],
   "source": [
    "anno_df_google = filter_google_play_links(\"done_googleplay_en.csv\", column_name='url', output_path='clean_google_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbe8f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_apple_links(csv_path, column_name='url', output_path=None):\n",
    "    \"\"\"\n",
    "    read CSV, keep the data that fits https://apps.apple.coms format\n",
    "\n",
    "    parameters\n",
    "    1, csv_path (str): CSV file path\n",
    "    2, column_name (str)\n",
    "    3, output_path (str, optional)\n",
    "\n",
    "    return:\n",
    "    4, pd.DataFrame: clean DataFrame\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # check if col exist\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"‚ùå file name'{column_name}' not exist. CSV's columns' name includes{df.columns.tolist()}\")\n",
    "\n",
    "    # regulation\n",
    "    pattern = re.compile(r'^https:\\/\\/apps\\.apple\\.com')\n",
    "\n",
    "    # filter\n",
    "    mask = df[column_name].apply(lambda x: bool(pattern.match(str(x).strip())))\n",
    "    df_filtered = df[mask].copy()\n",
    "\n",
    "    # store the result csv file\n",
    "    if output_path:\n",
    "        df_filtered.to_csv(output_path, index=False)\n",
    "        print(f\"‚úÖ store Apple clean data to: {output_path}\")\n",
    "\n",
    "    print(f\"‚úÖ in total {len(df_filtered)} Apple corpus\")\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16cd051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ store Apple clean data to: clean_apple_en.csv\n",
      "‚úÖ in total 546 Apple corpus\n"
     ]
    }
   ],
   "source": [
    "anno_df_apple = filter_apple_links(\"done_apple_en.csv\", column_name='url', output_path='clean_apple_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9feaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###annotation by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a3f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_annotation_tool(df=None, csv_path=None, text_column='description', label_column='label', output_path='annotated_output.csv'):\n",
    "    # load data\n",
    "    if df is None:\n",
    "        if csv_path is None:\n",
    "            raise ValueError(\"Please provide df or csv_path\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        df = df.copy()\n",
    "\n",
    "    if label_column not in df.columns:\n",
    "        df[label_column] = \"\"\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    pending_df = df[df[label_column] == \"\"].reset_index()\n",
    "\n",
    "    if len(pending_df) == 0:\n",
    "        print(\"‚úÖ all data annotated! \")\n",
    "        return\n",
    "\n",
    "    current_idx = 0\n",
    "\n",
    "    # def button\n",
    "    btn_0 = widgets.Button(description=\"0 ‚ùå not relevant\", button_style='danger')\n",
    "    btn_1 = widgets.Button(description=\"1 ‚úÖ relevant\", button_style='success')\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def show_annotation(idx):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            if idx >= len(pending_df):\n",
    "                print(\"‚úÖ finished, storing...\")\n",
    "                df.to_csv(output_path, index=False)\n",
    "                print(f\"‚úÖ stored to: {output_path}\")\n",
    "                return\n",
    "            row_idx = pending_df.loc[idx, 'index']\n",
    "            text = df.loc[row_idx, text_column]\n",
    "            print(f\"üìÑ No. {idx + 1}  / in total {len(pending_df)} \\n\")\n",
    "            print(text)\n",
    "\n",
    "    def on_button_clicked(label_value):\n",
    "        nonlocal current_idx\n",
    "        if current_idx >= len(pending_df):\n",
    "            return\n",
    "        row_idx = pending_df.loc[current_idx, 'index']\n",
    "        df.at[row_idx, label_column] = label_value\n",
    "        current_idx += 1\n",
    "        show_annotation(current_idx)\n",
    "\n",
    "    btn_0.on_click(lambda b: on_button_clicked(0))\n",
    "    btn_1.on_click(lambda b: on_button_clicked(1))\n",
    "\n",
    "    display(widgets.HBox([btn_0, btn_1]), output)\n",
    "    show_annotation(current_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df273ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcfb9eef2df4755b8cd1e6aa3572109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='0 ‚ùå not relevant', style=ButtonStyle()), Button(butt‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6905f805ed6246b79f2a6a3266e867d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "launch_annotation_tool(csv_path='clean_google_en.csv', output_path='annotated__google.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05f0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_label_review(\n",
    "    csv_path,\n",
    "    text_column='description',\n",
    "    label_column='label',\n",
    "    output_path='corrected_output.csv'\n",
    "):\n",
    "    \"\"\"\n",
    "    loop through csv file, check if there is sth need to be modify, and save.\n",
    "    \"\"\"\n",
    "    with open('annotated__google.csv', encoding='utf-8', errors='ignore') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    current = {'i': 0}\n",
    "\n",
    "    btn_keep = widgets.Button(description=\"Keep the same\")\n",
    "    btn_set_0 = widgets.Button(description=\"modify to 0\", button_style='danger')\n",
    "    btn_set_1 = widgets.Button(description=\"modify to 1\", button_style='success')\n",
    "    btn_save = widgets.Button(description=\"save result\", button_style='info')\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "\n",
    "    def show_item():\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            if current['i'] >= len(df):\n",
    "                print(\"‚úÖ finished loop through\")\n",
    "                print(\"Please click save botton to save the modification.\")\n",
    "                return\n",
    "            try:\n",
    "                text = df.loc[current['i'], text_column]\n",
    "                label = df.loc[current['i'], label_column]\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå error while reading data {e}\")\n",
    "                return\n",
    "\n",
    "            print(f\"üìÑ No. {current['i'] + 1}  / in total {len(df)} \")\n",
    "            print(f\"text context:\\n{text}\\n\")\n",
    "            print(f\"current label: {label}\")\n",
    "            print(\"Please choose: \")\n",
    "\n",
    "    def keep_label(b):\n",
    "        current['i'] += 1\n",
    "        show_item()\n",
    "\n",
    "    def set_label_0(b):\n",
    "        df.at[current['i'], label_column] = 0\n",
    "        current['i'] += 1\n",
    "        show_item()\n",
    "\n",
    "    def set_label_1(b):\n",
    "        df.at[current['i'], label_column] = 1\n",
    "        current['i'] += 1\n",
    "        show_item()\n",
    "\n",
    "    def save_result(b):\n",
    "        df.to_csv(output_path, index=False)\n",
    "        with output_area:\n",
    "            print(f\"‚úÖ modification is saved to {output_path}\")\n",
    "\n",
    "    btn_keep.on_click(keep_label)\n",
    "    btn_set_0.on_click(set_label_0)\n",
    "    btn_set_1.on_click(set_label_1)\n",
    "    btn_save.on_click(save_result)\n",
    "\n",
    "    buttons_box = widgets.HBox([btn_keep, btn_set_0, btn_set_1, btn_save])\n",
    "\n",
    "    display(output_area, buttons_box)\n",
    "    show_item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b5055f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d47ca67c0604a659e11beda2bb46cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb06dd68c1a4f2d8b391c7aeb39370b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Keep the same', style=ButtonStyle()), Button(button_style='danger', descrip‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_label_review('annotated__google.csv', text_column='description', label_column='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03625712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Google Play for: ISO 9001\n",
      "Searching Google Play for: ISO 13485\n",
      "Searching Google Play for: ISO 27001\n",
      "Searching Google Play for: EU-GDPR\n",
      "  √ó Search failed for keyword 'EU-GDPR': 'ds:4'\n",
      "‚úÖ Saved data to test_googleplay_en.csv, total apps after deduplication: 59\n"
     ]
    }
   ],
   "source": [
    "test_keywords = [ \"ISO 9001\",\"ISO 13485\",\"ISO 27001\",\"EU-GDPR\"]\n",
    "df_GxP_google = fetch_google_play_apps(test_keywords, lang='en', country='us', file_suffix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c891af35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e04d95cb994bb89b64ae40345a32ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='0 ‚ùå not relevant', style=ButtonStyle()), Button(butt‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99eb3c36cd0847009db36b7b07657e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "launch_annotation_tool(csv_path='test_googleplay_en.csv', output_path='annotated_test_google_GxP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a13765",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_csv('annotated_test_google_GxP.csv')\n",
    "# f2 = pd.read_csv('annotated__google.csv')\n",
    "with open('annotated__google.csv', encoding='utf-8', errors='ignore') as f:\n",
    "    f2 = pd.read_csv(f)   \n",
    "merged = pd.concat([f1, f2], ignore_index=True)\n",
    "merged.to_csv('annotated__google.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
